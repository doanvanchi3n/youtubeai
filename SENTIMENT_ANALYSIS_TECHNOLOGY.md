# üî¨ C√îNG NGH·ªÜ PH√ÇN LO·∫†I B√åNH LU·∫¨N - CHI TI·∫æT K·ª∏ THU·∫¨T

## üìö T·ªîNG QUAN C√îNG NGH·ªÜ

### **C√¥ng ngh·ªá ch√≠nh:**
- **Machine Learning Framework**: **scikit-learn** (Python)
- **Ph∆∞∆°ng ph√°p**: **Text Classification** (Supervised Learning)
- **M√¥ h√¨nh**: **Pipeline** k·∫øt h·ª£p TF-IDF Vectorization + Classifier
- **Th∆∞ vi·ªán h·ªó tr·ª£**: **underthesea** (Vietnamese NLP), **joblib** (Model serialization)

---

## üèóÔ∏è KI·∫æN TR√öC M√î H√åNH

### **1. Sentiment Analysis Model (3 l·ªõp)**
```
Input: B√¨nh lu·∫≠n text
  ‚Üì
Preprocessing (TextProcessor)
  ‚Üì
TF-IDF Vectorization (chuy·ªÉn text ‚Üí s·ªë)
  ‚Üì
Classifier (SVM/Naive Bayes/Logistic Regression)
  ‚Üì
Output: positive | negative | neutral
```

### **2. Emotion Classification Model (5 l·ªõp)**
```
Input: B√¨nh lu·∫≠n text
  ‚Üì
Preprocessing (TextProcessor)
  ‚Üì
TF-IDF Vectorization
  ‚Üì
Classifier
  ‚Üì
Output: happy | sad | angry | suggestion | love
```

---

## üîß CHI TI·∫æT C√îNG NGH·ªÜ

### **A. TEXT PREPROCESSING (Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n)**

**M·ª•c ƒë√≠ch**: Chu·∫©n h√≥a text ƒë·ªÉ model d·ªÖ h·ªçc h∆°n

**C√°c b∆∞·ªõc:**

#### **1. Lowercase Conversion**
```python
# Input: "Video N√ÄY R·∫§T HAY!"
# Output: "video n√†y r·∫•t hay!"
text = text.lower()
```

#### **2. Remove URLs & Emails**
```python
# Input: "Xem th√™m t·∫°i https://youtube.com/watch?v=abc"
# Output: "Xem th√™m t·∫°i "
text = re.sub(r'http\S+|www\S+', '', text)
text = re.sub(r'\S+@\S+', '', text)
```

#### **3. Remove Extra Whitespace**
```python
# Input: "Video    n√†y    r·∫•t    hay"
# Output: "Video n√†y r·∫•t hay"
text = re.sub(r'\s+', ' ', text)
text = text.strip()
```

**V√≠ d·ª• th·ª±c t·∫ø:**
```python
# Comment g·ªëc:
"Video n√†y R·∫§T HAY!!! Xem t·∫°i https://youtube.com/watch?v=abc    Tuy·ªát v·ªùi!"

# Sau preprocessing:
"video n√†y r·∫•t hay!!! xem t·∫°i tuy·ªát v·ªùi!"
```

---

### **B. TF-IDF VECTORIZATION (Chuy·ªÉn ƒë·ªïi text ‚Üí s·ªë)**

**TF-IDF l√† g√¨?**
- **TF (Term Frequency)**: T·∫ßn su·∫•t t·ª´ xu·∫•t hi·ªán trong document
- **IDF (Inverse Document Frequency)**: ƒê·ªô hi·∫øm c·ªßa t·ª´ trong to√†n b·ªô corpus
- **M·ª•c ƒë√≠ch**: Chuy·ªÉn text th√†nh vector s·ªë ƒë·ªÉ ML model c√≥ th·ªÉ x·ª≠ l√Ω

**V√≠ d·ª• minh h·ªça:**

#### **B∆∞·ªõc 1: T·∫°o Vocabulary t·ª´ Training Data**
```python
# Gi·∫£ s·ª≠ c√≥ 3 comments trong training data:
comments = [
    "video n√†y r·∫•t hay",
    "video n√†y kh√¥ng hay",
    "video n√†y b√¨nh th∆∞·ªùng"
]

# Vocabulary (t·ª´ ƒëi·ªÉn):
vocabulary = {
    "video": 0,
    "n√†y": 1,
    "r·∫•t": 2,
    "hay": 3,
    "kh√¥ng": 4,
    "b√¨nh": 5,
    "th∆∞·ªùng": 6
}
```

#### **B∆∞·ªõc 2: T√≠nh TF-IDF cho m·ªói comment**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=5000,      # T·ªëi ƒëa 5000 t·ª´ quan tr·ªçng nh·∫•t
    ngram_range=(1, 2),     # Unigram + Bigram (1 t·ª´, 2 t·ª´)
    min_df=2,               # T·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 2 l·∫ßn
    max_df=0.95             # B·ªè qua t·ª´ xu·∫•t hi·ªán > 95% documents
)

# Fit v√† transform
X = vectorizer.fit_transform(comments)

# K·∫øt qu·∫£: M·ªói comment ‚Üí vector s·ªë (sparse matrix)
# V√≠ d·ª• comment "video n√†y r·∫•t hay":
# [0.5, 0.3, 0.8, 0.6, 0.0, 0.0, 0.0, ...]
#  ‚Üë     ‚Üë    ‚Üë    ‚Üë
# video n√†y r·∫•t hay
```

**V√≠ d·ª• c·ª• th·ªÉ:**
```python
# Comment: "Video n√†y r·∫•t hay"
# Vector (simplified):
{
    "video": 0.3,    # TF-IDF score
    "n√†y": 0.2,
    "r·∫•t": 0.5,      # T·ª´ "r·∫•t" quan tr·ªçng (tƒÉng c∆∞·ªùng √Ω nghƒ©a)
    "hay": 0.8,      # T·ª´ "hay" r·∫•t quan tr·ªçng (t·ª´ kh√≥a ch√≠nh)
    "video_n√†y": 0.4,  # Bigram
    "r·∫•t_hay": 0.9     # Bigram quan tr·ªçng nh·∫•t
}
```

---

### **C. CLASSIFIER (B·ªô ph√¢n lo·∫°i)**

**C√°c thu·∫≠t to√°n ph·ªï bi·∫øn:**

#### **1. Naive Bayes (MultinomialNB)**
- **∆Øu ƒëi·ªÉm**: Nhanh, hi·ªáu qu·∫£ v·ªõi text classification
- **C√°ch ho·∫°t ƒë·ªông**: T√≠nh x√°c su·∫•t t·ª´ng l·ªõp d·ª±a tr√™n Bayes Theorem

```python
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB(alpha=1.0)  # alpha = smoothing parameter
model.fit(X_train, y_train)        # X_train: TF-IDF vectors, y_train: labels

# Predict
prediction = model.predict(X_test)
# Output: "positive"
```

#### **2. Support Vector Machine (SVM)**
- **∆Øu ƒëi·ªÉm**: Ch√≠nh x√°c cao, x·ª≠ l√Ω t·ªët v·ªõi nhi·ªÅu features
- **C√°ch ho·∫°t ƒë·ªông**: T√¨m ƒë∆∞·ªùng bi√™n t·ªëi ∆∞u ƒë·ªÉ ph√¢n t√°ch c√°c l·ªõp

```python
from sklearn.svm import SVC

model = SVC(kernel='linear', C=1.0)
model.fit(X_train, y_train)
```

#### **3. Logistic Regression**
- **∆Øu ƒëi·ªÉm**: Nhanh, d·ªÖ interpret, cho probability scores
- **C√°ch ho·∫°t ƒë·ªông**: S·ª≠ d·ª•ng sigmoid function ƒë·ªÉ t√≠nh x√°c su·∫•t

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict v·ªõi probability
probabilities = model.predict_proba(X_test)
# Output: [0.1, 0.8, 0.1] ‚Üí 80% positive, 10% negative, 10% neutral
```

---

## üìä V√ç D·ª§ TH·ª∞C T·∫æ - T·ª™NG B∆Ø·ªöC

### **V√≠ d·ª• 1: Ph√¢n t√≠ch Sentiment**

#### **Input:**
```python
comment = "Video n√†y qu√° tuy·ªát v·ªùi! C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu ‚ù§Ô∏è"
```

#### **B∆∞·ªõc 1: Preprocessing**
```python
from app.utils.text_processor import TextProcessor

processor = TextProcessor()
processed = processor.preprocess(comment)

# K·∫øt qu·∫£:
# "video n√†y qu√° tuy·ªát v·ªùi! c·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu ‚ù§Ô∏è"
```

#### **B∆∞·ªõc 2: Vectorization**
```python
# Model ƒë√£ ƒë∆∞·ª£c train, c√≥ s·∫µn vectorizer
# Transform comment th√†nh vector
vector = vectorizer.transform([processed])

# Vector (simplified):
# {
#   "video": 0.2,
#   "n√†y": 0.1,
#   "tuy·ªát": 0.7,      # T·ª´ t√≠ch c·ª±c m·∫°nh
#   "v·ªùi": 0.6,
#   "c·∫£m_∆°n": 0.8,     # Bigram t√≠ch c·ª±c
#   "r·∫•t": 0.4,
#   "nhi·ªÅu": 0.3,
#   "tuy·ªát_v·ªùi": 0.9   # Bigram r·∫•t t√≠ch c·ª±c
# }
```

#### **B∆∞·ªõc 3: Prediction**
```python
# Sentiment Model
sentiment_model = model_loader.get_sentiment_model()

# Predict
sentiment = sentiment_model.predict(vector)[0]
# Output: "positive"

# Predict v·ªõi probability
proba = sentiment_model.predict_proba(vector)[0]
# Output: [0.05, 0.90, 0.05]
#         ‚Üë     ‚Üë     ‚Üë
#      negative positive neutral
# Confidence: 90% positive
```

#### **B∆∞·ªõc 4: Emotion Classification**
```python
# Emotion Model
emotion_model = model_loader.get_emotion_model()

emotion = emotion_model.predict(vector)[0]
# Output: "love" (v√¨ c√≥ "c·∫£m ∆°n", "‚ù§Ô∏è")

proba = emotion_model.predict_proba(vector)[0]
# Output: [0.1, 0.1, 0.1, 0.1, 0.6]
#         ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë
#      happy sad angry suggestion love
# Confidence: 60% love
```

#### **K·∫øt qu·∫£ cu·ªëi c√πng:**
```json
{
  "sentiment": "positive",
  "emotion": "love",
  "confidence": 0.75  // (0.90 + 0.60) / 2
}
```

---

### **V√≠ d·ª• 2: Comment ti√™u c·ª±c**

#### **Input:**
```python
comment = "Video n√†y ch√°n qu√°, kh√¥ng hay g√¨ c·∫£ üòû"
```

#### **B∆∞·ªõc 1: Preprocessing**
```python
processed = "video n√†y ch√°n qu√°, kh√¥ng hay g√¨ c·∫£ üòû"
```

#### **B∆∞·ªõc 2: Vectorization**
```python
# Vector highlights:
# {
#   "ch√°n": 0.8,        # T·ª´ ti√™u c·ª±c m·∫°nh
#   "kh√¥ng": 0.6,       # Ph·ªß ƒë·ªãnh
#   "hay": 0.3,         # Nh∆∞ng b·ªã ph·ªß ƒë·ªãnh
#   "ch√°n_qu√°": 0.9,    # Bigram ti√™u c·ª±c
#   "kh√¥ng_hay": 0.7    # Bigram ph·ªß ƒë·ªãnh
# }
```

#### **B∆∞·ªõc 3: Prediction**
```python
sentiment = "negative"  # Confidence: 85%
emotion = "sad"         # Confidence: 70%
```

---

### **V√≠ d·ª• 3: Comment trung l·∫≠p**

#### **Input:**
```python
comment = "C√≥ th·ªÉ c·∫£i thi·ªán ph·∫ßn √¢m thanh m·ªôt ch√∫t"
```

#### **K·∫øt qu·∫£:**
```python
sentiment = "neutral"   # Kh√¥ng t√≠ch c·ª±c, kh√¥ng ti√™u c·ª±c
emotion = "suggestion"   # ƒê∆∞a ra g√≥p √Ω
```

---

## üéì QUY TR√åNH TRAINING MODEL

### **B∆∞·ªõc 1: Chu·∫©n b·ªã Training Data**

**Format CSV:**
```csv
text,label
"Video n√†y r·∫•t hay",positive
"Video n√†y kh√¥ng hay",negative
"C√≥ th·ªÉ c·∫£i thi·ªán",neutral
"Tuy·ªát v·ªùi qu√°",positive
...
```

**V√≠ d·ª• dataset:**
```python
# Sentiment Training Data (3 classes)
sentiment_data = [
    ("Video n√†y r·∫•t hay", "positive"),
    ("Tuy·ªát v·ªùi qu√°", "positive"),
    ("Video n√†y kh√¥ng hay", "negative"),
    ("Ch√°n qu√°", "negative"),
    ("C√≥ th·ªÉ c·∫£i thi·ªán", "neutral"),
    ("B√¨nh th∆∞·ªùng", "neutral"),
    # ... h√†ng ngh√¨n examples
]

# Emotion Training Data (5 classes)
emotion_data = [
    ("Video n√†y r·∫•t hay", "happy"),
    ("C·∫£m ∆°n b·∫°n", "love"),
    ("Ch√°n qu√°", "sad"),
    ("T·ª©c gi·∫≠n", "angry"),
    ("C√≥ th·ªÉ c·∫£i thi·ªán", "suggestion"),
    # ... h√†ng ngh√¨n examples
]
```

### **B∆∞·ªõc 2: Training Pipeline**

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# T√°ch features v√† labels
X = [text for text, label in data]
y = [label for text, label in data]

# T√°ch train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# T·∫°o pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95
    )),
    ('classifier', MultinomialNB(alpha=1.0))
])

# Training
pipeline.fit(X_train, y_train)

# Evaluate
accuracy = pipeline.score(X_test, y_test)
print(f"Accuracy: {accuracy:.2%}")

# Save model
import joblib
joblib.dump(pipeline, 'sentiment_model.pkl')
```

### **B∆∞·ªõc 3: Model Evaluation**

```python
from sklearn.metrics import classification_report, confusion_matrix

# Predict test set
y_pred = pipeline.predict(X_test)

# Classification Report
print(classification_report(y_test, y_pred))

# Output:
#               precision    recall  f1-score   support
#     negative       0.85      0.82      0.83       200
#      neutral       0.78      0.80      0.79       150
#     positive       0.88      0.90      0.89       250
#     accuracy                           0.85       600
#    macro avg       0.84      0.84      0.84       600
```

---

## üíª CODE IMPLEMENTATION TRONG D·ª∞ √ÅN

### **File: `ai_module/app/services/sentiment_service.py`**

```python
from app.models.model_loader import ModelLoader
from app.utils.text_processor import TextProcessor

model_loader = ModelLoader()
text_processor = TextProcessor()

class SentimentService:
    def analyze(self, text):
        # 1. Preprocess
        processed_text = text_processor.preprocess(text)
        # "Video n√†y R·∫§T HAY!!!" ‚Üí "video n√†y r·∫•t hay!!!"
        
        # 2. Get models
        sentiment_model = model_loader.get_sentiment_model()
        emotion_model = model_loader.get_emotion_model()
        
        # 3. Predict sentiment
        sentiment_pred = sentiment_model.predict([processed_text])[0]
        # Output: "positive"
        
        sentiment_proba = sentiment_model.predict_proba([processed_text])[0]
        # Output: [0.05, 0.90, 0.05] ‚Üí max = 0.90
        sentiment_confidence = max(sentiment_proba)
        
        # 4. Predict emotion
        emotion_pred = emotion_model.predict([processed_text])[0]
        # Output: "happy"
        
        emotion_proba = emotion_model.predict_proba([processed_text])[0]
        emotion_confidence = max(emotion_proba)
        
        # 5. Return result
        return {
            'sentiment': sentiment_pred,      # "positive"
            'emotion': emotion_pred,          # "happy"
            'confidence': (sentiment_confidence + emotion_confidence) / 2
        }
```

### **File: `ai_module/app/api/sentiment.py`**

```python
from flask import Blueprint, request, jsonify
from app.services.sentiment_service import SentimentService

bp = Blueprint('sentiment', __name__)
sentiment_service = SentimentService()

@bp.route('/analyze-sentiment', methods=['POST'])
def analyze_sentiment():
    data = request.get_json()
    text = data['text']
    
    # Analyze
    result = sentiment_service.analyze(text)
    
    return jsonify(result), 200

# Example request:
# POST http://localhost:5000/api/analyze-sentiment
# Body: { "text": "Video n√†y r·∫•t hay!" }
# Response: { "sentiment": "positive", "emotion": "happy", "confidence": 0.85 }
```

---

## üîÑ LU·ªíNG X·ª¨ L√ù TRONG H·ªÜ TH·ªêNG

### **Khi Backend nh·∫≠n comment m·ªõi:**

```
1. Backend (Java) l∆∞u comment v√†o DB
   Comment {
     content: "Video n√†y r·∫•t hay!",
     is_analyzed: false
   }

2. Backend g·ªçi AI Module (Python Flask)
   POST http://localhost:5000/api/analyze-sentiment
   Body: { "text": "Video n√†y r·∫•t hay!" }

3. AI Module x·ª≠ l√Ω:
   a. Preprocess: "video n√†y r·∫•t hay!"
   b. TF-IDF Vectorization
   c. Sentiment Model ‚Üí "positive" (90% confidence)
   d. Emotion Model ‚Üí "happy" (80% confidence)

4. AI Module tr·∫£ v·ªÅ:
   {
     "sentiment": "positive",
     "emotion": "happy",
     "confidence": 0.85
   }

5. Backend c·∫≠p nh·∫≠t DB:
   Comment {
     sentiment: "positive",
     emotion: "happy",
     sentiment_score: 0.85,
     is_analyzed: true,
     analyzed_at: "2024-01-15 10:30:00"
   }
```

---

## üìà C·∫¢I THI·ªÜN ƒê·ªò CH√çNH X√ÅC

### **1. TƒÉng ch·∫•t l∆∞·ª£ng Training Data**
- Thu th·∫≠p nhi·ªÅu comments th·ª±c t·∫ø t·ª´ YouTube
- C√¢n b·∫±ng s·ªë l∆∞·ª£ng m·ªói l·ªõp (balanced dataset)
- Label ch√≠nh x√°c (human annotation)

### **2. Feature Engineering**
- Th√™m emoji detection (‚ù§Ô∏è ‚Üí love, üòû ‚Üí sad)
- Th√™m negation handling ("kh√¥ng hay" ‚Üí negative)
- S·ª≠ d·ª•ng word embeddings (Word2Vec, FastText)

### **3. Model Tuning**
```python
# Grid Search ƒë·ªÉ t√¨m hyperparameters t·ªët nh·∫•t
from sklearn.model_selection import GridSearchCV

param_grid = {
    'tfidf__max_features': [3000, 5000, 10000],
    'classifier__alpha': [0.5, 1.0, 1.5]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
```

### **4. Advanced Models (Optional)**
- **Transformers**: BERT, PhoBERT (Vietnamese BERT)
- **Deep Learning**: LSTM, BiLSTM v·ªõi word embeddings
- **Ensemble**: K·∫øt h·ª£p nhi·ªÅu models

---

## üéØ T√ìM T·∫ÆT

### **C√¥ng ngh·ªá:**
- **Framework**: scikit-learn (Python)
- **Method**: Text Classification v·ªõi TF-IDF + Classifier
- **Models**: 2 models ri√™ng bi·ªát (Sentiment 3 l·ªõp, Emotion 5 l·ªõp)

### **Quy tr√¨nh:**
1. **Preprocessing** ‚Üí Chu·∫©n h√≥a text
2. **Vectorization** ‚Üí Chuy·ªÉn text ‚Üí s·ªë (TF-IDF)
3. **Classification** ‚Üí Model d·ª± ƒëo√°n l·ªõp
4. **Confidence** ‚Üí T√≠nh ƒë·ªô tin c·∫≠y

### **V√≠ d·ª•:**
- "Video n√†y r·∫•t hay!" ‚Üí positive, happy (85% confidence)
- "Ch√°n qu√°" ‚Üí negative, sad (80% confidence)
- "C√≥ th·ªÉ c·∫£i thi·ªán" ‚Üí neutral, suggestion (75% confidence)

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

- [scikit-learn Text Classification](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)
- [TF-IDF Explained](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
- [Naive Bayes for Text Classification](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [Vietnamese NLP with underthesea](https://github.com/undertheseanlp/underthesea)


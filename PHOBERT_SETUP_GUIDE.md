# üîß H∆Ø·ªöNG D·∫™N SETUP PHOBERT MODELS

## üìã T·ªïng quan

AI Module h·ªó tr·ª£ 2 ch·∫ø ƒë·ªô:
1. **PhoBERT** (∆Øu ti√™n): Pre-trained model cho ti·∫øng Vi·ªát, ƒë·ªô ch√≠nh x√°c cao
2. **scikit-learn** (Fallback): S·ª≠ d·ª•ng n·∫øu PhoBERT models ch∆∞a c√≥

## ‚ö° QUICK START (Nhanh nh·∫•t)

### B∆∞·ªõc 1: C√†i dependencies
```bash
cd ai_module
pip install transformers>=4.40.0 torch>=2.2.0
```

Ho·∫∑c tr√™n Windows:
```bash
cd ai_module
install_phobert.bat
```

### B∆∞·ªõc 2: Setup models t·ª± ƒë·ªông
```bash
cd ai_module
python setup_phobert_quick.py
```

Script n√†y s·∫Ω:
- T·∫°o th∆∞ m·ª•c models
- Download base PhoBERT
- T·∫°o sentiment model (3 classes)
- T·∫°o emotion model (5 classes)

‚ö†Ô∏è **L∆∞u √Ω**: Models n√†y ch∆∞a ƒë∆∞·ª£c fine-tune, ch·ªâ l√† base models. ƒê·ªÉ c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n, c·∫ßn fine-tune v·ªõi training data (xem b√™n d∆∞·ªõi).

---

## üöÄ C√°ch 1: Download Pre-trained Models (Khuy·∫øn ngh·ªã)

### B∆∞·ªõc 1: T·∫°o th∆∞ m·ª•c models
```bash
cd ai_module/app/data/models
mkdir -p phobert_sentiment phobert_emotion
```

### B∆∞·ªõc 2: Download ho·∫∑c Fine-tune Models

**Option A: S·ª≠ d·ª•ng models c√≥ s·∫µn (n·∫øu c√≥)**
```bash
# Copy models v√†o th∆∞ m·ª•c
cp /path/to/phobert_sentiment/* phobert_sentiment/
cp /path/to/phobert_emotion/* phobert_emotion/
```

**Option B: Fine-tune t·ª´ PhoBERT base**
```python
# File: ai_module/train_phobert.py
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset
import torch

# Load base model
model_name = "vinai/phobert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load your training data
# Format: CSV with columns: text, label
train_dataset = load_dataset('csv', data_files='training_data/sentiment_data.csv')

# Fine-tune for sentiment (3 classes: negative, neutral, positive)
sentiment_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# Training arguments
training_args = TrainingArguments(
    output_dir='./phobert_sentiment',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=500,
    logging_steps=100,
)

trainer = Trainer(
    model=sentiment_model,
    args=training_args,
    train_dataset=train_dataset['train'],
)

trainer.train()
trainer.save_model('./phobert_sentiment')

# T∆∞∆°ng t·ª± cho emotion model (5 classes)
```

## üîÑ C√°ch 2: S·ª≠ d·ª•ng scikit-learn (Fallback)

N·∫øu ch∆∞a c√≥ PhoBERT models, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông fallback v·ªÅ scikit-learn.

**T·∫°o models scikit-learn:**
```bash
cd ai_module
python app/scripts/train_sentiment.py --data app/data/training_data/sentiment_data.csv
python app/scripts/train_emotion.py --data app/data/training_data/emotion_data.csv
```

Models s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i:
- `app/data/models/sentiment_model.pkl`
- `app/data/models/emotion_model.pkl`

## ‚úÖ Ki·ªÉm tra Models

Sau khi setup, ki·ªÉm tra models:

```python
# Test script
from app.services.sentiment_service import SentimentService

service = SentimentService()
print(f"Using PhoBERT: {service.use_phobert}")

result = service.analyze("Video n√†y r·∫•t hay!")
print(result)
```

## üìù Environment Variables (Optional)

C√≥ th·ªÉ c·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n models qua environment variables:

```bash
export PHOBERT_SENTIMENT_MODEL_PATH=app/data/models/phobert_sentiment
export PHOBERT_EMOTION_MODEL_PATH=app/data/models/phobert_emotion
```

Ho·∫∑c trong `.env`:
```
PHOBERT_SENTIMENT_MODEL_PATH=app/data/models/phobert_sentiment
PHOBERT_EMOTION_MODEL_PATH=app/data/models/phobert_emotion
```

## üéØ K·∫øt qu·∫£ mong ƒë·ª£i

Khi models ƒë∆∞·ª£c load th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y trong logs:
```
‚úì PhoBERT models loaded successfully
```

N·∫øu ch∆∞a c√≥ models:
```
‚ö† PhoBERT models not found at app/data/models/phobert_sentiment
  Using scikit-learn fallback
```

## üìö T√†i li·ªáu tham kh·∫£o

- [PhoBERT GitHub](https://github.com/VinAIResearch/PhoBERT)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training)

